# 工作描述

# 2020/7/7

- 构思结构

- 写了一个微信小程序的页面，用于学生接受导师分阶段发来的信息

# 2020/7/8

- 学习爬虫的使用
- 构思运维功能的部分结构以及部分实现设想

# 2020/7/9

- 明确了部分功能目标
- 本日大部分时间都花在研究bysj.jwc.sjtu.edu.cn这个网站的结构上了，目前还在测试发post请求之后利用session进行下一步操作（但是遇见了一些BUG） 同时怕自己出现奇怪的操作造成某些不可挽回的影响所以做的比较慢

# 2020/7/10

- 对网页结构的分析 抓包后发现爬虫的实现途径应为如下：首先构造DATA和HEADER，此后创建一个session发post请求,在接受response的cookie之后加入header后向登陆后跳转的界面进行第二次请求，此后同样将response.cookie加入直接发起get请求获取表单数据即可
- 但是在实现过程中发现由于重定向问题导致了很多奇奇怪怪的BUG(分别用python的requests库、Postman以及浏览器抓包)得到了三种结果，尤其是抓包之后发现这个POST的status_code是200而重定向的status_code应该为302，故卡在这里很久。后换用了selenium做发现效果非常显著

# 2020/7/12

- 写了一个脚本，用于将脚本爬出来的数据利用正则表达式提取出我们所需要的信息，由于网页页面的构造略有些特殊 因此对于每一页爬出来的数据用不同的文件保存(否则处理起来将会十分难受)
- 写了对于开题报告、中期检查等阶段的爬虫脚本，至于对于文件内容的处理待明天上班开工后与队友沟通之后再进行解决

# 2020/7/13

- 与队友构思爬取数据需要提取哪部分，对于数据库添加了一个新表，用于储存网页爬取的数据，同时考虑到已有数据库部分表的结构，故对其进行了优化减少了冗余。上午并对Python脚本结构进行了构思(主要是我将插入数据的脚本写成了一个DataManager的类,但是写的并不是很理想,对于插入数据的函数以及构造函数我个人认为都有很大改进空间,但是本人对于Python的类并不是很熟悉 还在摸索)
- 学习了一下JUnit,配置了一个Maven Project并添加了JUnit的依赖，进行了一个小小的测试，但是对于在实习项目中如何去使用JUnit仍然存在着一定的困扰。
- 继续学习正则表达式，以提取爬取数据。一开始写了一个正则表达式，适用于大部分数据，但是面对小部分数据的时候还是会有错误，故主要把时间耗费在了写正则表达式这件事上。
- 实际操作了pull requests

# 2020/7/14

- 正则表达式需要更新，由于有部分老师的工号不是数字，因此需要用更复杂的方法来提取

  初版方法:

  提取汉字: ``pattern1 = re.compile(r'[\u4e00-\u9fa5]+')`` 

  提取学号/工号:``pattern3 = re.compile(r'\d{5,13}') ``

  已在处理时遇见异常抛出，需要进行改进。

  改版之后:

  提取学生名字(单独一行处理):由于可能有留学生等, 因此我们不妨考虑到第一行的特点: 一个数字+空格+学生姓名,故我们只需要将该字符串利用spilt(' ',1)分割之后取后半部分即可.

  提取学号工号:利用非贪婪的正则表达式`` p1 = re.compile(r'[(](.*?)[)]', re.S)``找到对应括号的内容 考虑到论文题目本身可能会带有括号 因此先取第一个作为学生学号 再逆序取最后一个作为老师工号

  提取阶段状态以及老师名字: 与初版一致,不需要做过多处理

- 提取数据之前首先我们先对student和teacher的table做一次判断 若为空.我们需要对这两个表插入数据(以便于当学生/老师注册微信小程序时的检索),同样地,若这个表为空,证明我们进行的是第一次爬取,此时我们对于info表的操作应该是insert而不是update.否则我们对于相应的sql改成update即可.

- 完成了大部分脚本,准备复用正则表达式时发现不同阶段爬出来的数据格式还不同...正在奋力研究中

- 已经完成了全部的python脚本,且测试后运行效果良好,暂无BUG.

# 2020/7/15

- 测试了昨天编写脚本,在原有数据库已有内容下的更新功能,检测发现更新功能实现良好,暂无BUG
- 编写python脚本,用于生成其他几个数据库的随机测试数据(之前爬取出来的数据是用于更新info以及初次爬取连带填充student和tutor表的)
- 发现了原有插入数据脚本的部分错误:由于自己是分文件插入的,因此我应该把文件夹的遍历放入函数当中去做这个处理,正在debug中.
- 对于定期爬虫更新的数据,研究了一下如何使用python触发后端接口:半天触发一次爬虫,此后检测是否有状态更新.如果有状态变化更新,则推送给服务号
- 研究了一下python脚本如何挂载服务器(尤其是我们这个是使用了selenium 如何挂载浏览器驱动)
- 对于部分后端的接口进行了Unit test

# 2020/7/16

- 编写了一个python脚本,用于在爬虫爬取数据之后,利用data文件夹里面的文件与现有数据库中的信息相检测,如果有变化,则需要发送消息(如何调用接口,怎么实现?可能还是需要等服务号批下来后进一步构造)
- 编写了数据字典
- 学习了如何将python脚本与服务号关联,利用自己的个人服务号进行了一些测试,等待服务号批下来之后进行进一步的研究.(更正:应为企业微信)

# 2020/7/17

- 编写了自己负责的微信小程序页面的后端请求逻辑
- 挂在了一个python脚本到本地(利用ngrok映射到外网),并测试了一个比较基础的脚本(首先接受微信发来的get请求,通过query_string拿到微信发送的参数,再通过wechatpy的check_signature函数来校验。校验成功后把echostr返回给微信)
- 学习了部分微信公众号的api, 明确了如何操作当我们爬虫注入并且刷新到有数据变化时:首先根据该生的学号到student表中找到对应的uid(也即openid),此后向微信的接口发一个get请求去拿到access_token,拿到后再向微信的另一个接口发出POST请求,即可向对应的uid的微信用户发出消息

# 2020/7/20

- 将之前写的python脚本均封装成类,以便后期挂在or测试时集成,顺便学了一下python的面向对象,不愧是高级语言,博大精深

# 2020/7/21

- 封装类
- 学习Auth相关知识,拟以微信的api的行为模式:先get_access_token,之后再在发请求的时候带上这个token来实现我们对于后端接口的改动,即OAuth2.0的模式
- 学习之后拟采取JWT token来完成token验证的过程
- 电脑环境出了点问题 正在重新配置环境

# 2020/7/22

- 学习了Spring Security的相关知识,自己照着网上的教程写了一个小测试(存在BUG)
- 拟采用JWT的方案,编写了一个JWTUtils的工具类,做了一部分测试
- 学习了JWT的原理,了解了部分知识